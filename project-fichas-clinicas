# src/train_loop.py
from __future__ import annotations
"""
Entrenamiento supervisado con validaci√≥n y early stopping.


- Carga train/val desde data/processed (CSV)
- Crea el modelo v√≠a src.model.make_model(cfg)
- LOGREG: fit √∫nico + evaluaci√≥n en val
- MLP: entrenamiento incremental (warm_start) con early stopping (patience=10)
- Guarda:
    - results/model.joblib  (mejor checkpoint por AUROC_val)
    - results/train_log.csv (hist√≥rico por √©poca)
"""

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import joblib
import numpy as np
import pandas as pd
from sklearn.metrics import average_precision_score, roc_auc_score

from src.model import default_config, make_model


@dataclass
class TrainConfig:
    model_cfg: Dict[str, Any]
    max_epochs: int = 100
    patience: int = 10
    outdir: Path = Path("results")


def _load_xy(csv_path: Path, target: str = "label") -> Tuple[pd.DataFrame, pd.Series]:
    df = pd.read_csv(csv_path)
    if target not in df.columns:
        raise ValueError(f"No encuentro la columna objetivo '{target}' en {csv_path}")
    y = df[target].astype(int)
    X = df.drop(columns=[target])
    return X, y


def _class_weight_sample_weights(y: pd.Series) -> Optional[np.ndarray]:
    """
    Calcula weights por clase para pasar como sample_weight a estimadores
    que no soportan class_weight directamente (como MLPClassifier).
    Devuelve None si hay problema.
    """
    try:
        classes, counts = np.unique(y, return_counts=True)
        freq = dict(zip(classes.tolist(), counts.tolist()))
        if len(freq) < 2:
            return None  # no se puede ponderar una sola clase
        total = len(y)
        # weight_c = total / (num_clases * count_c)
        weights = {c: total / (len(freq) * cnt) for c, cnt in freq.items()}
        return np.array([weights[int(t)] for t in y.tolist()], dtype=float)
    except Exception:
        return None


def _eval_metrics(y_true: np.ndarray, proba: np.ndarray) -> Dict[str, float]:
    """
    Calcula AUROC y AUPRC. Maneja casos edge cuando no hay ambas clases.
    """
    out: Dict[str, float] = {}
    try:
        out["auroc"] = roc_auc_score(y_true, proba)
    except Exception:
        out["auroc"] = float("nan")
    try:
        out["auprc"] = average_precision_score(y_true, proba)
    except Exception:
        out["auprc"] = float("nan")
    return out


def _save_log(log_rows: list[Dict[str, Any]], out_csv: Path) -> None:
    df = pd.DataFrame(log_rows)
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=False)


def _save_model(model, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, out_path)


def _is_mlp(model) -> bool:
    return model.__class__.__name__.lower().startswith("mlp")


def train_and_validate(cfg: TrainConfig, data_dir: Path, target: str = "label") -> None:
    # --- carga datos ---
    Xtr, ytr = _load_xy(data_dir / "train.csv", target=target)
    Xva, yva = _load_xy(data_dir / "val.csv", target=target)

    # --- construye modelo ---
    model = make_model(cfg.model_cfg)

    # --- rutas de salida ---
    outdir = cfg.outdir
    outdir.mkdir(parents=True, exist_ok=True)
    model_path = outdir / "model.joblib"
    log_path = outdir / "train_log.csv"

    log_rows: list[Dict[str, Any]] = []

    if not _is_mlp(model):
        # ----- LOGREG (u otro estimador no incremental): fit √∫nico -----
        model.fit(Xtr, ytr)
        proba_va = model.predict_proba(Xva)[:, 1]
        metrics_va = _eval_metrics(yva.values, proba_va)
        log_rows.append({"epoch": 1, **metrics_va})
        _save_log(log_rows, log_path)
        _save_model(model, model_path)
        print(f"‚úî Modelo guardado en: {model_path}")
        print(f"Val AUROC={metrics_va['auroc']:.4f} AUPRC={metrics_va['auprc']:.4f}")
        return

    # ----- MLP con early stopping manual e incremental -----
    # Configuramos para entrenamiento por √©pocas controladas:
    # - max_iter=1 y warm_start=True para avanzar 1 "√©poca" por fit()
    # - usamos sample_weight para class balance
    params = model.get_params()
    if params.get("max_iter", 1) != 1:
        model.set_params(max_iter=1)
    if not params.get("warm_start", False):
        try:
            model.set_params(warm_start=True)
        except Exception:
            pass  # algunos estimadores podr√≠an no soportar warm_start

    # Ponderaci√≥n por clase
    sw = _class_weight_sample_weights(ytr)

    best_auroc = -np.inf
    best_epoch = 0
    best_state: Optional[bytes] = None
    epochs_no_improve = 0

    for epoch in range(1, cfg.max_epochs + 1):
        # Una "√©poca" de actualizaci√≥n
        try:
            model.fit(Xtr, ytr, sample_weight=sw)  # para MLPClassifier es v√°lido
        except TypeError:
            # fallback si no acepta sample_weight
            model.fit(Xtr, ytr)

        # Eval en validaci√≥n
        proba_va = model.predict_proba(Xva)[:, 1]
        metrics_va = _eval_metrics(yva.values, proba_va)
        log_rows.append({"epoch": epoch, **metrics_va})

        # Early stopping por AUROC
        auroc = metrics_va["auroc"]
        if np.isfinite(auroc) and auroc > best_auroc:
            best_auroc = auroc
            best_epoch = epoch
            # guardamos snapshot binario del modelo (pickle) en memoria
            best_state = joblib.dumps(model)
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        print(
            f"Epoch {epoch:03d} | Val AUROC={metrics_va['auroc']:.4f} "
            f"AUPRC={metrics_va['auprc']:.4f} | "
            f"Best@{best_epoch}={best_auroc:.4f} | "
            f"no_improve={epochs_no_improve}/{cfg.patience}"
        )

        if epochs_no_improve >= cfg.patience:
            print("‚èπ Early stopping activado.")
            break

    # Guardar log y mejor checkpoint
    _save_log(log_rows, log_path)
    if best_state is not None:
        best_model = joblib.loads(best_state)
        _save_model(best_model, model_path)
        print(f"‚úî Mejor modelo (epoch {best_epoch}) guardado en: {model_path}")
    else:
        # si nunca mejor√≥, guardamos el modelo actual
        _save_model(model, model_path)
        print("‚ö† No hubo mejora; guardado el modelo final.")

    print(f"üìÑ Log de entrenamiento: {log_path}")


def _load_json(path: Optional[str]) -> Dict[str, Any]:
    if not path:
        return {}
    p = Path(path)
    with open(p, "r", encoding="utf-8") as f:
        return json.load(f)
=======
# src/model.py
from __future__ import annotations
"""
Definici√≥n de modelos:
- Regresi√≥n Log√≠stica (baseline)
- MLPClassifier (red densa con backprop)
Uso:
    from src.model import make_model, default_config
    cfg = default_config(model="mlp")  # o "logreg"
    model = make_model(cfg)
    model.fit(X_train, y_train)
    y_proba = model.predict_proba(X_val)[:, 1]
"""

from typing import Any, Dict

from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier


def default_config(model: str = "mlp") -> Dict[str, Any]:
    """
    Config por defecto para cada modelo.
    - model: "logreg" | "mlp"
    """
    if model == "logreg":
        return {
            "model": "logreg",
            "logreg": {
                "max_iter": 200,
                "class_weight": "balanced",
                "solver": "lbfgs",
                "n_jobs": None,  # usa todos si None y solver lo permite
                "random_state": 42,
            },
        }
    if model == "mlp":
        return {
            "model": "mlp",
            "mlp": {
                "hidden_layer_sizes": [128, 64],
                "activation": "relu",
                "solver": "adam",
                "alpha": 1e-4,               # L2
                "batch_size": "auto",
                "learning_rate": "adaptive",  # adapta LR si se estanca
                "learning_rate_init": 1e-3,
                "max_iter": 100,
                "early_stopping": True,
                "n_iter_no_change": 10,       # paciencia (val)
                "validation_fraction": 0.1,   # solo lo usa internamente el MLP
                "shuffle": True,
                "random_state": 42,
                "verbose": False,
            },
        }
    raise ValueError("Modelo no soportado. Usa 'logreg' o 'mlp'.")


def make_model(cfg: Dict[str, Any]):
    """
    Crea el estimador sklearn seg√∫n cfg.
    cfg ejemplo:
    {
        "model": "mlp",
        "mlp": {
            "hidden_layer_sizes": [128, 64],
            "max_iter": 100,
            "early_stopping": True,
            ...
        }
    }
    """
    model_name = cfg.get("model", "mlp").lower()

    if model_name == "logreg":
        p = (cfg.get("logreg") or {}).copy()
        # Defaults seguros si faltan
        p.setdefault("max_iter", 200)
        p.setdefault("class_weight", "balanced")
        p.setdefault("solver", "lbfgs")
        p.setdefault("n_jobs", None)
        p.setdefault("random_state", 42)
        return LogisticRegression(**p)

    if model_name == "mlp":
        p = (cfg.get("mlp") or {}).copy()
        # Defaults seguros si faltan
        p.setdefault("hidden_layer_sizes", [128, 64])
        p.setdefault("activation", "relu")
        p.setdefault("solver", "adam")
        p.setdefault("alpha", 1e-4)
        p.setdefault("batch_size", "auto")
        p.setdefault("learning_rate", "adaptive")
        p.setdefault("learning_rate_init", 1e-3)
        p.setdefault("max_iter", 100)
        p.setdefault("early_stopping", True)
        p.setdefault("n_iter_no_change", 10)
        p.setdefault("validation_fraction", 0.1)
        p.setdefault("shuffle", True)
        p.setdefault("random_state", 42)
        p.setdefault("verbose", False)

        # Acepta listas para hidden_layer_sizes
        if isinstance(p.get("hidden_layer_sizes"), list):
            p["hidden_layer_sizes"] = tuple(p["hidden_layer_sizes"])

        return MLPClassifier(**p)

    raise ValueError("Modelo no soportado. Usa 'logreg' o 'mlp'.")


def get_supported_models() -> Dict[str, str]:
    """Devuelve un mapa simple de modelos soportados."""
    return {
        "logreg": "LogisticRegression (baseline, lineal)",
        "mlp": "MLPClassifier (red densa con backprop, no lineal)",
    }
=======
# src/preprocess.py
from __future__ import annotations
"""
Preprocesamiento tabular:
- Split estratificado en train/val/test
- Imputaci√≥n (num: mediana, cat: "unknown")
- Escalado num√©rico (z-score)
- One-hot en categ√≥ricas (handle_unknown="ignore")
- Guardado/carga del preprocesador con joblib
"""

from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple

import joblib
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler


@dataclass
class SplitData:
    X_train: pd.DataFrame
    y_train: pd.Series
    X_val: pd.DataFrame
    y_val: pd.Series
    X_test: pd.DataFrame
    y_test: pd.Series


def stratified_split(
    df: pd.DataFrame,
    target: str,
    val_size: float = 0.15,
    test_size: float = 0.15,
    random_state: int = 42,
) -> SplitData:
    """Hace split estratificado en train/val/test manteniendo proporciones."""
    y = df[target]
    X = df.drop(columns=[target])

    X_train, X_tmp, y_train, y_tmp = train_test_split(
        X,
        y,
        test_size=val_size + test_size,
        stratify=y,
        random_state=random_state,
    )
    rel_test = test_size / (val_size + test_size)
    X_val, X_test, y_val, y_test = train_test_split(
        X_tmp,
        y_tmp,
        test_size=rel_test,
        stratify=y_tmp,
        random_state=random_state,
    )
    return SplitData(X_train, y_train, X_val, y_val, X_test, y_test)


def infer_columns(
    df: pd.DataFrame,
    target: str,
    numeric_hint: List[str] | None = None,
    categorical_hint: List[str] | None = None,
) -> Tuple[List[str], List[str]]:
    """
    Infiera columnas num√©ricas y categ√≥ricas.
    Puedes forzar con *_hint*. El target se excluye siempre.
    """
    cols = [c for c in df.columns if c != target]
    if numeric_hint is not None or categorical_hint is not None:
        num = [c for c in (numeric_hint or []) if c in cols]
        cat = [c for c in (categorical_hint or []) if c in cols]
        rest = [c for c in cols if c not in num and c not in cat]
        # resto: asume categ√≥rico
        cat.extend(rest)
        return num, cat

    # inferencia simple por dtype
    num = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    cat = [c for c in cols if c not in num]
    return num, cat


def make_preprocessor(
    numeric_cols: List[str],
    categorical_cols: List[str],
) -> ColumnTransformer:
    """Crea el ColumnTransformer con imputaci√≥n + escalado/one-hot."""
    num_pipe = make_numeric_pipeline()
    cat_pipe = make_categorical_pipeline()
    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, numeric_cols),
            ("cat", cat_pipe, categorical_cols),
        ]
    )
    return pre


def make_numeric_pipeline():
    """Imputaci√≥n mediana + escalado est√°ndar."""
    from sklearn.pipeline import Pipeline
    return Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )


def make_categorical_pipeline():
    """Imputaci√≥n 'unknown' + one-hot (ignora categor√≠as nuevas)."""
    from sklearn.pipeline import Pipeline
    return Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="constant", fill_value="unknown")),
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
        ]
    )


def fit_transform_all(
    split: SplitData,
    numeric_cols: List[str],
    categorical_cols: List[str],
    preprocessor_path: str | Path = "results/preprocessor.joblib",
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Ajusta el preprocesador en train y transforma train/val/test.
    Guarda el preprocesador en disco para inferencia posterior.
    """
    pre = make_preprocessor(numeric_cols, categorical_cols)

    Xtr = pre.fit_transform(split.X_train)
    Xva = pre.transform(split.X_val)
    Xte = pre.transform(split.X_test)

    # nombres de columnas resultantes
    ohe = pre.named_transformers_["cat"]["ohe"]  # type: ignore[index]
    cat_names = list(ohe.get_feature_names_out(categorical_cols))
    num_names = numeric_cols
    out_cols = num_names + cat_names

    Xtr = pd.DataFrame(Xtr, columns=out_cols, index=split.X_train.index)
    Xva = pd.DataFrame(Xva, columns=out_cols, index=split.X_val.index)
    Xte = pd.DataFrame(Xte, columns=out_cols, index=split.X_test.index)

    Path(preprocessor_path).parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(pre, preprocessor_path)

    return Xtr, Xva, Xte


def load_preprocessor(preprocessor_path: str | Path):
    """Carga el preprocesador guardado en disco."""
    return joblib.load(preprocessor_path)


def transform_with_loaded(
    preprocessor,
    X: pd.DataFrame,
    numeric_cols: List[str],
    categorical_cols: List[str],
) -> pd.DataFrame:
    """Transforma un DataFrame con un preprocesador ya cargado."""
    Xt = preprocessor.transform(X)
    ohe = preprocessor.named_transformers_["cat"]["ohe"]  # type: ignore[index]
    cat_names = list(ohe.get_feature_names_out(categorical_cols))
    out_cols = list(numeric_cols) + cat_names
    return pd.DataFrame(Xt, columns=out_cols, index=X.index)



if __name__ == "__main__":
    import argparse

    ap = argparse.ArgumentParser(
        description="Entrenamiento con validaci√≥n y early stopping (MLP)."
    )
    ap.add_argument(
        "--data-dir",
        default="data/processed",
        help="Carpeta con train.csv y val.csv (por defecto data/processed).",
    )
    ap.add_argument(
        "--config",
        default="",
        help="Ruta a JSON con configuraci√≥n del modelo (opcional).",
    )
    ap.add_argument(
        "--model",
        default="mlp",
        choices=["mlp", "logreg"],
        help="Modelo por defecto si no se provee config JSON.",
    )
    ap.add_argument(
        "--max-epochs",
        type=int,
        default=100,
        help="M√°ximo de √©pocas para MLP (default=100).",
    )
    ap.add_argument(
        "--patience",
        type=int,
        default=10,
        help="Paciencia para early stopping por AUROC (default=10).",
    )
    ap.add_argument(
        "--outdir",
        default="results",
        help="Carpeta de salida para modelo y logs (default=results).",
    )
    args = ap.parse_args()

    # carga cfg
    cfg_json = _load_json(args.config)
    if cfg_json:
        model_cfg = cfg_json
    else:
        model_cfg = default_config(args.model)

    train_cfg = TrainConfig(
        model_cfg=model_cfg,
        max_epochs=int(args.max_epochs),
        patience=int(args.patience),
        outdir=Path(args.outdir),
    )

    train_and_validate(train_cfg, Path(args.data_dir))
        description="Preprocesa CSV tabular (split + transform) y guarda artefactos.",
    )
    ap.add_argument("--csv", required=True, help="Ruta al CSV de entrada.")
    ap.add_argument("--target", default="label", help="Nombre de la columna objetivo.")
    ap.add_argument(
        "--outdir",
        default="data/processed",
        help="Carpeta de salida para CSVs y preprocesador.",
    )
    args = ap.parse_args()

    df_in = pd.read_csv(args.csv)
    num_cols, cat_cols = infer_columns(df_in, target=args.target)

    split = stratified_split(df_in, target=args.target)
    Xtr, Xva, Xte = fit_transform_all(
        split, num_cols, cat_cols, preprocessor_path=Path(args.outdir) / "preprocessor.joblib"
    )

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    pd.concat([Xtr, split.y_train], axis=1).to_csv(outdir / "train.csv", index=False)
    pd.concat([Xva, split.y_val], axis=1).to_csv(outdir / "val.csv", index=False)
    pd.concat([Xte, split.y_test], axis=1).to_csv(outdir / "test.csv", index=False)

    print("‚úî Guardado preprocesador en:", outdir / "preprocessor.joblib")
    print("‚úî train/val/test en:", outdir)
