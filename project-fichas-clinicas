# src/api.py
from __future__ import annotations
"""
API de inferencia con FastAPI.

Endpoints:
- GET  /health        -> estado del servicio
- GET  /schema        -> columnas crudas esperadas por el preprocesador
- POST /predict       -> probabilidad y clase (umbral configurable)

Requisitos (artefactos entrenados):
- results/model.joblib
- results/preprocessor.joblib
"""

from pathlib import Path
from typing import Any, Dict, List, Optional
# src/evaluate.py
from __future__ import annotations
"""
Evaluación en TEST:
- Carga results/model.joblib
- Lee data/processed/test.csv
- Calcula AUROC/AUPRC y métricas a umbral
- Guarda ROC.png, PR.png y test_metrics.json
"""

from pathlib import Path
from typing import Dict, Tuple

import joblib
import numpy as np
import pandas as pd

from src.metrics import (
    compute_classification_metrics,
    confusion_counts,
    plot_pr,
    plot_roc,
    save_json,
)
# src/metrics.py
from __future__ import annotations
"""
Métricas y gráficos para clasificación binaria.

Funciones:
- compute_classification_metrics: AUROC, AUPRC y métricas a umbral (acc/prec/recall/F1)
- confusion_counts: TP, FP, TN, FN
- plot_roc: guarda curva ROC
- plot_pr: guarda curva Precision-Recall
- save_json: escribe un dict en JSON
"""

from pathlib import Path
from typing import Dict, Tuple

import json
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    confusion_matrix,
    f1_score,
    precision_recall_curve,
    precision_score,
    recall_score,
    roc_auc_score,
    roc_curve,
)
import matplotlib.pyplot as plt


def compute_classification_metrics(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    threshold: float = 0.5,
) -> Dict[str, float]:
    """Calcula AUROC, AUPRC y métricas a un umbral dado."""
    out: Dict[str, float] = {}

    # Probabilidades bien formadas
    y_true = np.asarray(y_true).astype(int)
    y_proba = np.asarray(y_proba).astype(float)

    # Métricas umbraladas
    y_pred = (y_proba >= float(threshold)).astype(int)

    # Métricas robustas (maneja excepciones si solo hay una clase)
    try:
        out["auroc"] = roc_auc_score(y_true, y_proba)
    except Exception:
        out["auroc"] = float("nan")

    try:
        out["auprc"] = average_precision_score(y_true, y_proba)
    except Exception:
        out["auprc"] = float("nan")

    out["accuracy"] = accuracy_score(y_true, y_pred)
    out["precision"] = precision_score(y_true, y_pred, zero_division=0)
    out["recall"] = recall_score(y_true, y_pred, zero_division=0)
    out["f1"] = f1_score(y_true, y_pred, zero_division=0)

    return out


def confusion_counts(
    y_true: np.ndarray,
    y_pred: np.ndarray,
) -> Dict[str, int]:
    """Retorna TP, FP, TN, FN en un dict."""
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return {"tp": int(tp), "fp": int(fp), "tn": int(tn), "fn": int(fn)}


def plot_roc(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    out_path: Path,
) -> None:
    """Guarda curva ROC en out_path (PNG)."""
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    try:
        auc = roc_auc_score(y_true, y_proba)
    except Exception:
        auc = float("nan")

    plt.figure()
    plt.plot(fpr, tpr, label=f"ROC AUC = {auc:.3f}")
    plt.plot([0, 1], [0, 1], linestyle="--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend(loc="lower right")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, bbox_inches="tight", dpi=150)
    plt.close()


def plot_pr(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    out_path: Path,
) -> None:
    """Guarda curva Precision-Recall en out_path (PNG)."""
    precision, recall, _ = precision_recall_curve(y_true, y_proba)
    try:
        auprc = average_precision_score(y_true, y_proba)
    except Exception:
        auprc = float("nan")

    plt.figure()
    plt.plot(recall, precision, label=f"AUPRC = {auprc:.3f}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.legend(loc="lower left")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, bbox_inches="tight", dpi=150)
    plt.close()


def save_json(data: Dict, out_path: Path) -> None:
    """Guarda un diccionario en JSON (con indentación)."""
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def table_from_metrics(d: Dict[str, float]) -> pd.DataFrame:
    """Convierte el dict de métricas a DataFrame de una fila (útil para logs)."""
    return pd.D
# src/train_loop.py
from __future__ import annotations
"""
Entrenamiento supervisado con validación y early stopping.


- Carga train/val desde data/processed (CSV)
- Crea el modelo vía src.model.make_model(cfg)
- LOGREG: fit único + evaluación en val
- MLP: entrenamiento incremental (warm_start) con early stopping (patience=10)
- Guarda:
    - results/model.joblib  (mejor checkpoint por AUROC_val)
    - results/train_log.csv (histórico por época)
"""

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import joblib
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel, Field

MODEL_PATH = Path("results/model.joblib")
PREPROC_PATH = Path("results/preprocessor.joblib")

app = FastAPI(title="Clinical ML Inference API", version="1.0.0")


class PredictRequest(BaseModel):
    features: Dict[str, Any] = Field(..., description="Mapa columna -> valor")


class PredictResponse(BaseModel):
    ok: bool
    proba: float
    label: int
    threshold: float
    missing: List[str] = []
    extra: List[str] = []


def _load_artifacts():
    if not MODEL_PATH.exists():
        raise FileNotFoundError(f"No se encontró el modelo en {MODEL_PATH}")
    if not PREPROC_PATH.exists():
        raise FileNotFoundError(f"No se encontró el preprocesador en {PREPROC_PATH}")
    model = joblib.load(MODEL_PATH)
    pre = joblib.load(PREPROC_PATH)
    return model, pre


def _infer_raw_columns_from_preprocessor(pre) -> Dict[str, List[str]]:
    numeric_cols: List[str] = []
    categorical_cols: List[str] = []
    for name, _trans, cols in pre.transformers_:
        if name == "remainder":
            continue
        if not isinstance(cols, list):
            try:
                if hasattr(pre, "feature_names_in_"):
                    cols = list(pre.feature_names_in_)
                else:
                    cols = []
            except Exception:
                cols = []
        if name == "num":
            numeric_cols.extend([str(c) for c in cols])
        elif name == "cat":
            categorical_cols.extend([str(c) for c in cols])
    return {"numeric": numeric_cols, "categorical": categorical_cols}


try:
    _MODEL, _PREPROC = _load_artifacts()
    _RAW_SCHEMA = _infer_raw_columns_from_preprocessor(_PREPROC)
    _ALIVE = True
except Exception as e:  # noqa: BLE001
    _MODEL, _PREPROC, _RAW_SCHEMA = None, None, {"numeric": [], "categorical": []}
    _ALIVE = False
    _STARTUP_ERROR = str(e)


@app.get("/health")
def health():
    return {
        "ok": _ALIVE,
        "model_path": str(MODEL_PATH),
        "preprocessor_path": str(PREPROC_PATH),
        "error": None if _ALIVE else _STARTUP_ERROR,
    }


@app.get("/schema")
def schema():
    if not _ALIVE:
        raise HTTPException(status_code=503, detail=_STARTUP_ERROR)
    return _RAW_SCHEMA


def _dataframe_from_features(
    features: Dict[str, Any],
    raw_schema: Dict[str, List[str]],
) -> pd.DataFrame:
    """
    Construye un DataFrame de UNA fila con las columnas crudas esperadas.
    Llena faltantes con None; deja extras para reportarlas.
    """
    cols = list(raw_schema["numeric"]) + list(raw_schema["categorical"])
    data: Dict[str, List[Any]] = {}
    for c in cols:
        data[c] = [features.get(c, None)]
    df = pd.DataFrame(data)
    return df


@app.post("/predict", response_model=PredictResponse)
def predict(
    req: PredictRequest,
    threshold: float = Query(0.5, ge=0.0, le=1.0, description="Umbral para clasificar"),
):
    if not _ALIVE:
        raise HTTPException(status_code=503, detail=_STARTUP_ERROR)

    # Construye DF crudo
    df_raw = _dataframe_from_features(req.features, _RAW_SCHEMA)

    # Detecta faltantes y extras (solo informativo)
    expected = set(_RAW_SCHEMA["numeric"] + _RAW_SCHEMA["categorical"])
    got = set(req.features.keys())
    missing = sorted(list(expected - got))
    extra = sorted(list(got - expected))

    # Transforma
    try:
        X = _PREPROC.transform(df_raw)
    except Exception as e:  # noqa: BLE001
        raise HTTPException(status_code=400, detail=f"Error en preprocesamiento: {e}")

    # Predice
    try:
        proba = float(_MODEL.predict_proba(X)[:, 1][0])
    except Exception as e:  # noqa: BLE001
        raise HTTPException(status_code=500, detail=f"Error en predicción: {e}")

    label = int(proba >= float(threshold))
    return PredictResponse(
        ok=True,
        proba=proba,
        label=label,
        threshold=float(threshold),
        missing=missing,
        extra=extra,
    )
from sklearn.metrics import average_precision_score, roc_auc_score

from src.model import default_config, make_model


@dataclass
class TrainConfig:
    model_cfg: Dict[str, Any]
    max_epochs: int = 100
    patience: int = 10
    outdir: Path = Path("results")


def _load_xy(csv_path: Path, target: str = "label") -> Tuple[pd.DataFrame, pd.Series]:
    df = pd.read_csv(csv_path)
    if target not in df.columns:
        raise ValueError(f"No encuentro la columna objetivo '{target}' en {csv_path}")
    y = df[target].astype(int)
    X = df.drop(columns=[target])
    return X, y


def evaluate_model(
    model_path: Path,
    test_csv: Path,
    outdir: Path,
    threshold: float = 0.5,
) -> Dict:
    """Evalúa el modelo en test y guarda artefactos."""
    outdir.mkdir(parents=True, exist_ok=True)

    # Carga modelo y datos
    model = joblib.load(model_path)
    Xte, yte = _load_xy(test_csv, target="label")

    # Predicciones
    proba = model.predict_proba(Xte)[:, 1]
    y_pred = (proba >= threshold).astype(int)

    # Métricas
    metrics = compute_classification_metrics(yte.values, proba, threshold=threshold)
    conf = confusion_counts(yte.values, y_pred)

    # Gráficos
    plot_roc(yte.values, proba, outdir / "roc_curve.png")
    plot_pr(yte.values, proba, outdir / "pr_curve.png")

    # Guarda JSON
    out = {
        "threshold": float(threshold),
        "metrics": metrics,
        "confusion": conf,
        "n_test": int(len(yte)),
    }
    save_json(out, outdir / "test_metrics.json")
    return out
def _class_weight_sample_weights(y: pd.Series) -> Optional[np.ndarray]:
    """
    Calcula weights por clase para pasar como sample_weight a estimadores
    que no soportan class_weight directamente (como MLPClassifier).
    Devuelve None si hay problema.
    """
    try:
        classes, counts = np.unique(y, return_counts=True)
        freq = dict(zip(classes.tolist(), counts.tolist()))
        if len(freq) < 2:
            return None  # no se puede ponderar una sola clase
        total = len(y)
        # weight_c = total / (num_clases * count_c)
        weights = {c: total / (len(freq) * cnt) for c, cnt in freq.items()}
        return np.array([weights[int(t)] for t in y.tolist()], dtype=float)
    except Exception:
        return None


def _eval_metrics(y_true: np.ndarray, proba: np.ndarray) -> Dict[str, float]:
    """
    Calcula AUROC y AUPRC. Maneja casos edge cuando no hay ambas clases.
    """
    out: Dict[str, float] = {}
    try:
        out["auroc"] = roc_auc_score(y_true, proba)
    except Exception:
        out["auroc"] = float("nan")
    try:
        out["auprc"] = average_precision_score(y_true, proba)
    except Exception:
        out["auprc"] = float("nan")
    return out


def _save_log(log_rows: list[Dict[str, Any]], out_csv: Path) -> None:
    df = pd.DataFrame(log_rows)
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=False)


def _save_model(model, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, out_path)


def _is_mlp(model) -> bool:
    return model.__class__.__name__.lower().startswith("mlp")


def train_and_validate(cfg: TrainConfig, data_dir: Path, target: str = "label") -> None:
    # --- carga datos ---
    Xtr, ytr = _load_xy(data_dir / "train.csv", target=target)
    Xva, yva = _load_xy(data_dir / "val.csv", target=target)

    # --- construye modelo ---
    model = make_model(cfg.model_cfg)

    # --- rutas de salida ---
    outdir = cfg.outdir
    outdir.mkdir(parents=True, exist_ok=True)
    model_path = outdir / "model.joblib"
    log_path = outdir / "train_log.csv"

    log_rows: list[Dict[str, Any]] = []

    if not _is_mlp(model):
        # ----- LOGREG (u otro estimador no incremental): fit único -----
        model.fit(Xtr, ytr)
        proba_va = model.predict_proba(Xva)[:, 1]
        metrics_va = _eval_metrics(yva.values, proba_va)
        log_rows.append({"epoch": 1, **metrics_va})
        _save_log(log_rows, log_path)
        _save_model(model, model_path)
        print(f"✔ Modelo guardado en: {model_path}")
        print(f"Val AUROC={metrics_va['auroc']:.4f} AUPRC={metrics_va['auprc']:.4f}")
        return

    # ----- MLP con early stopping manual e incremental -----
    # Configuramos para entrenamiento por épocas controladas:
    # - max_iter=1 y warm_start=True para avanzar 1 "época" por fit()
    # - usamos sample_weight para class balance
    params = model.get_params()
    if params.get("max_iter", 1) != 1:
        model.set_params(max_iter=1)
    if not params.get("warm_start", False):
        try:
            model.set_params(warm_start=True)
        except Exception:
            pass  # algunos estimadores podrían no soportar warm_start

    # Ponderación por clase
    sw = _class_weight_sample_weights(ytr)

    best_auroc = -np.inf
    best_epoch = 0
    best_state: Optional[bytes] = None
    epochs_no_improve = 0

    for epoch in range(1, cfg.max_epochs + 1):
        # Una "época" de actualización
        try:
            model.fit(Xtr, ytr, sample_weight=sw)  # para MLPClassifier es válido
        except TypeError:
            # fallback si no acepta sample_weight
            model.fit(Xtr, ytr)

        # Eval en validación
        proba_va = model.predict_proba(Xva)[:, 1]
        metrics_va = _eval_metrics(yva.values, proba_va)
        log_rows.append({"epoch": epoch, **metrics_va})

        # Early stopping por AUROC
        auroc = metrics_va["auroc"]
        if np.isfinite(auroc) and auroc > best_auroc:
            best_auroc = auroc
            best_epoch = epoch
            # guardamos snapshot binario del modelo (pickle) en memoria
            best_state = joblib.dumps(model)
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        print(
            f"Epoch {epoch:03d} | Val AUROC={metrics_va['auroc']:.4f} "
            f"AUPRC={metrics_va['auprc']:.4f} | "
            f"Best@{best_epoch}={best_auroc:.4f} | "
            f"no_improve={epochs_no_improve}/{cfg.patience}"
        )

        if epochs_no_improve >= cfg.patience:
            print("⏹ Early stopping activado.")
            break

    # Guardar log y mejor checkpoint
    _save_log(log_rows, log_path)
    if best_state is not None:
        best_model = joblib.loads(best_state)
        _save_model(best_model, model_path)
        print(f"✔ Mejor modelo (epoch {best_epoch}) guardado en: {model_path}")
    else:
        # si nunca mejoró, guardamos el modelo actual
        _save_model(model, model_path)
        print("⚠ No hubo mejora; guardado el modelo final.")

    print(f"📄 Log de entrenamiento: {log_path}")


def _load_json(path: Optional[str]) -> Dict[str, Any]:
    if not path:
        return {}
    p = Path(path)
    with open(p, "r", encoding="utf-8") as f:
        return json.load(f)
# src/model.py
from __future__ import annotations
"""
Definición de modelos:
- Regresión Logística (baseline)
- MLPClassifier (red densa con backprop)
Uso:
    from src.model import make_model, default_config
    cfg = default_config(model="mlp")  # o "logreg"
    model = make_model(cfg)
    model.fit(X_train, y_train)
    y_proba = model.predict_proba(X_val)[:, 1]
"""

from typing import Any, Dict

from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier


def default_config(model: str = "mlp") -> Dict[str, Any]:
    """
    Config por defecto para cada modelo.
    - model: "logreg" | "mlp"
    """
    if model == "logreg":
        return {
            "model": "logreg",
            "logreg": {
                "max_iter": 200,
                "class_weight": "balanced",
                "solver": "lbfgs",
                "n_jobs": None,  # usa todos si None y solver lo permite
                "random_state": 42,
            },
        }
    if model == "mlp":
        return {
            "model": "mlp",
            "mlp": {
                "hidden_layer_sizes": [128, 64],
                "activation": "relu",
                "solver": "adam",
                "alpha": 1e-4,               # L2
                "batch_size": "auto",
                "learning_rate": "adaptive",  # adapta LR si se estanca
                "learning_rate_init": 1e-3,
                "max_iter": 100,
                "early_stopping": True,
                "n_iter_no_change": 10,       # paciencia (val)
                "validation_fraction": 0.1,   # solo lo usa internamente el MLP
                "shuffle": True,
                "random_state": 42,
                "verbose": False,
            },
        }
    raise ValueError("Modelo no soportado. Usa 'logreg' o 'mlp'.")


def make_model(cfg: Dict[str, Any]):
    """
    Crea el estimador sklearn según cfg.
    cfg ejemplo:
    {
        "model": "mlp",
        "mlp": {
            "hidden_layer_sizes": [128, 64],
            "max_iter": 100,
            "early_stopping": True,
            ...
        }
    }
    """
    model_name = cfg.get("model", "mlp").lower()

    if model_name == "logreg":
        p = (cfg.get("logreg") or {}).copy()
        # Defaults seguros si faltan
        p.setdefault("max_iter", 200)
        p.setdefault("class_weight", "balanced")
        p.setdefault("solver", "lbfgs")
        p.setdefault("n_jobs", None)
        p.setdefault("random_state", 42)
        return LogisticRegression(**p)

    if model_name == "mlp":
        p = (cfg.get("mlp") or {}).copy()
        # Defaults seguros si faltan
        p.setdefault("hidden_layer_sizes", [128, 64])
        p.setdefault("activation", "relu")
        p.setdefault("solver", "adam")
        p.setdefault("alpha", 1e-4)
        p.setdefault("batch_size", "auto")
        p.setdefault("learning_rate", "adaptive")
        p.setdefault("learning_rate_init", 1e-3)
        p.setdefault("max_iter", 100)
        p.setdefault("early_stopping", True)
        p.setdefault("n_iter_no_change", 10)
        p.setdefault("validation_fraction", 0.1)
        p.setdefault("shuffle", True)
        p.setdefault("random_state", 42)
        p.setdefault("verbose", False)

        # Acepta listas para hidden_layer_sizes
        if isinstance(p.get("hidden_layer_sizes"), list):
            p["hidden_layer_sizes"] = tuple(p["hidden_layer_sizes"])

        return MLPClassifier(**p)

    raise ValueError("Modelo no soportado. Usa 'logreg' o 'mlp'.")


def get_supported_models() -> Dict[str, str]:
    """Devuelve un mapa simple de modelos soportados."""
    return {
        "logreg": "LogisticRegression (baseline, lineal)",
        "mlp": "MLPClassifier (red densa con backprop, no lineal)",
    }
# src/preprocess.py
from __future__ import annotations
"""
Preprocesamiento tabular:
- Split estratificado en train/val/test
- Imputación (num: mediana, cat: "unknown")
- Escalado numérico (z-score)
- One-hot en categóricas (handle_unknown="ignore")
- Guardado/carga del preprocesador con joblib
"""

from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple

import joblib
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler


@dataclass
class SplitData:
    X_train: pd.DataFrame
    y_train: pd.Series
    X_val: pd.DataFrame
    y_val: pd.Series
    X_test: pd.DataFrame
    y_test: pd.Series


def stratified_split(
    df: pd.DataFrame,
    target: str,
    val_size: float = 0.15,
    test_size: float = 0.15,
    random_state: int = 42,
) -> SplitData:
    """Hace split estratificado en train/val/test manteniendo proporciones."""
    y = df[target]
    X = df.drop(columns=[target])

    X_train, X_tmp, y_train, y_tmp = train_test_split(
        X,
        y,
        test_size=val_size + test_size,
        stratify=y,
        random_state=random_state,
    )
    rel_test = test_size / (val_size + test_size)
    X_val, X_test, y_val, y_test = train_test_split(
        X_tmp,
        y_tmp,
        test_size=rel_test,
        stratify=y_tmp,
        random_state=random_state,
    )
    return SplitData(X_train, y_train, X_val, y_val, X_test, y_test)


def infer_columns(
    df: pd.DataFrame,
    target: str,
    numeric_hint: List[str] | None = None,
    categorical_hint: List[str] | None = None,
) -> Tuple[List[str], List[str]]:
    """
    Infiera columnas numéricas y categóricas.
    Puedes forzar con *_hint*. El target se excluye siempre.
    """
    cols = [c for c in df.columns if c != target]
    if numeric_hint is not None or categorical_hint is not None:
        num = [c for c in (numeric_hint or []) if c in cols]
        cat = [c for c in (categorical_hint or []) if c in cols]
        rest = [c for c in cols if c not in num and c not in cat]
        # resto: asume categórico
        cat.extend(rest)
        return num, cat

    # inferencia simple por dtype
    num = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]
    cat = [c for c in cols if c not in num]
    return num, cat


def make_preprocessor(
    numeric_cols: List[str],
    categorical_cols: List[str],
) -> ColumnTransformer:
    """Crea el ColumnTransformer con imputación + escalado/one-hot."""
    num_pipe = make_numeric_pipeline()
    cat_pipe = make_categorical_pipeline()
    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, numeric_cols),
            ("cat", cat_pipe, categorical_cols),
        ]
    )
    return pre


def make_numeric_pipeline():
    """Imputación mediana + escalado estándar."""
    from sklearn.pipeline import Pipeline
    return Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )


def make_categorical_pipeline():
    """Imputación 'unknown' + one-hot (ignora categorías nuevas)."""
    from sklearn.pipeline import Pipeline
    return Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="constant", fill_value="unknown")),
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
        ]
    )


def fit_transform_all(
    split: SplitData,
    numeric_cols: List[str],
    categorical_cols: List[str],
    preprocessor_path: str | Path = "results/preprocessor.joblib",
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Ajusta el preprocesador en train y transforma train/val/test.
    Guarda el preprocesador en disco para inferencia posterior.
    """
    pre = make_preprocessor(numeric_cols, categorical_cols)

    Xtr = pre.fit_transform(split.X_train)
    Xva = pre.transform(split.X_val)
    Xte = pre.transform(split.X_test)

    # nombres de columnas resultantes
    ohe = pre.named_transformers_["cat"]["ohe"]  # type: ignore[index]
    cat_names = list(ohe.get_feature_names_out(categorical_cols))
    num_names = numeric_cols
    out_cols = num_names + cat_names

    Xtr = pd.DataFrame(Xtr, columns=out_cols, index=split.X_train.index)
    Xva = pd.DataFrame(Xva, columns=out_cols, index=split.X_val.index)
    Xte = pd.DataFrame(Xte, columns=out_cols, index=split.X_test.index)

    Path(preprocessor_path).parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(pre, preprocessor_path)

    return Xtr, Xva, Xte


def load_preprocessor(preprocessor_path: str | Path):
    """Carga el preprocesador guardado en disco."""
    return joblib.load(preprocessor_path)


def transform_with_loaded(
    preprocessor,
    X: pd.DataFrame,
    numeric_cols: List[str],
    categorical_cols: List[str],
) -> pd.DataFrame:
    """Transforma un DataFrame con un preprocesador ya cargado."""
    Xt = preprocessor.transform(X)
    ohe = preprocessor.named_transformers_["cat"]["ohe"]  # type: ignore[index]
    cat_names = list(ohe.get_feature_names_out(categorical_cols))
    out_cols = list(numeric_cols) + cat_names
    return pd.DataFrame(Xt, columns=out_cols, index=X.index)



if __name__ == "__main__":
    import argparse
    import json

    ap = argparse.ArgumentParser(description="Evaluación en test.")
    ap.add_argument(
        "--model",
        default="results/model.joblib",
        help="Ruta al modelo entrenado (joblib).",
    )
    ap.add_argument(
        "--test-csv",
        default="data/processed/test.csv",
        help="CSV de test (features + label).",

    ap = argparse.ArgumentParser(
        description="Entrenamiento con validación y early stopping (MLP)."
    )
    ap.add_argument(
        "--data-dir",
        default="data/processed",
        help="Carpeta con train.csv y val.csv (por defecto data/processed).",
    )
    ap.add_argument(
        "--config",
        default="",
        help="Ruta a JSON con configuración del modelo (opcional).",
    )
    ap.add_argument(
        "--model",
        default="mlp",
        choices=["mlp", "logreg"],
        help="Modelo por defecto si no se provee config JSON.",
    )
    ap.add_argument(
        "--max-epochs",
        type=int,
        default=100,
        help="Máximo de épocas para MLP (default=100).",
    )
    ap.add_argument(
        "--patience",
        type=int,
        default=10,
        help="Paciencia para early stopping por AUROC (default=10).",
    )
    ap.add_argument(
        "--outdir",
        default="results",
        help="Carpeta de salida (gráficos y JSON).",
    )
    ap.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Umbral para métricas a clase (default=0.5).",
    )
    args = ap.parse_args()

    summary = evaluate_model(
        model_path=Path(args.model),
        test_csv=Path(args.test_csv),
        outdir=Path(args.outdir),
        threshold=float(args.threshold),
    )
    print(json.dumps(summary, indent=2))
        help="Carpeta de salida para modelo y logs (default=results).",
    )
    args = ap.parse_args()

    # carga cfg
    cfg_json = _load_json(args.config)
    if cfg_json:
        model_cfg = cfg_json
    else:
        model_cfg = default_config(args.model)

    train_cfg = TrainConfig(
        model_cfg=model_cfg,
        max_epochs=int(args.max_epochs),
        patience=int(args.patience),
        outdir=Path(args.outdir),
    )

    train_and_validate(train_cfg, Path(args.data_dir))
        description="Preprocesa CSV tabular (split + transform) y guarda artefactos.",
    )
    ap.add_argument("--csv", required=True, help="Ruta al CSV de entrada.")
    ap.add_argument("--target", default="label", help="Nombre de la columna objetivo.")
    ap.add_argument(
        "--outdir",
        default="data/processed",
        help="Carpeta de salida para CSVs y preprocesador.",
    )
    args = ap.parse_args()

    df_in = pd.read_csv(args.csv)
    num_cols, cat_cols = infer_columns(df_in, target=args.target)

    split = stratified_split(df_in, target=args.target)
    Xtr, Xva, Xte = fit_transform_all(
        split, num_cols, cat_cols, preprocessor_path=Path(args.outdir) / "preprocessor.joblib"
    )

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    pd.concat([Xtr, split.y_train], axis=1).to_csv(outdir / "train.csv", index=False)
    pd.concat([Xva, split.y_val], axis=1).to_csv(outdir / "val.csv", index=False)
    pd.concat([Xte, split.y_test], axis=1).to_csv(outdir / "test.csv", index=False)

    print("✔ Guardado preprocesador en:", outdir / "preprocessor.joblib")
    print("✔ train/val/test en:", outdir)
