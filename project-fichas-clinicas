# src/evaluate.py
from __future__ import annotations
"""
Evaluación en TEST:
- Carga results/model.joblib
- Lee data/processed/test.csv
- Calcula AUROC/AUPRC y métricas a umbral
- Guarda ROC.png, PR.png y test_metrics.json
"""

from pathlib import Path
from typing import Dict, Tuple

import joblib
import numpy as np
import pandas as pd

from src.metrics import (
    compute_classification_metrics,
    confusion_counts,
    plot_pr,
    plot_roc,
    save_json,
)


def _load_xy(csv_path: Path, target: str = "label") -> Tuple[pd.DataFrame, pd.Series]:
    df = pd.read_csv(csv_path)
    if target not in df.columns:
        raise ValueError(f"No encuentro la columna objetivo '{target}' en {csv_path}")
    y = df[target].astype(int)
    X = df.drop(columns=[target])
    return X, y


def evaluate_model(
    model_path: Path,
    test_csv: Path,
    outdir: Path,
    threshold: float = 0.5,
) -> Dict:
    """Evalúa el modelo en test y guarda artefactos."""
    outdir.mkdir(parents=True, exist_ok=True)

    # Carga modelo y datos
    model = joblib.load(model_path)
    Xte, yte = _load_xy(test_csv, target="label")

    # Predicciones
    proba = model.predict_proba(Xte)[:, 1]
    y_pred = (proba >= threshold).astype(int)

    # Métricas
    metrics = compute_classification_metrics(yte.values, proba, threshold=threshold)
    conf = confusion_counts(yte.values, y_pred)

    # Gráficos
    plot_roc(yte.values, proba, outdir / "roc_curve.png")
    plot_pr(yte.values, proba, outdir / "pr_curve.png")

    # Guarda JSON
    out = {
        "threshold": float(threshold),
        "metrics": metrics,
        "confusion": conf,
        "n_test": int(len(yte)),
    }
    save_json(out, outdir / "test_metrics.json")
    return out


if __name__ == "__main__":
    import argparse
    import json

    ap = argparse.ArgumentParser(description="Evaluación en test.")
    ap.add_argument(
        "--model",
        default="results/model.joblib",
        help="Ruta al modelo entrenado (joblib).",
    )
    ap.add_argument(
        "--test-csv",
        default="data/processed/test.csv",
        help="CSV de test (features + label).",
    )
    ap.add_argument(
        "--outdir",
        default="results",
        help="Carpeta de salida (gráficos y JSON).",
    )
    ap.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Umbral para métricas a clase (default=0.5).",
    )
    args = ap.parse_args()

    summary = evaluate_model(
        model_path=Path(args.model),
        test_csv=Path(args.test_csv),
        outdir=Path(args.outdir),
        threshold=float(args.threshold),
    )
    print(json.dumps(summary, indent=2))
